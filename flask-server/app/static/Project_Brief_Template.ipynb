{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# 2D Design Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "The purpose of this project is for you to apply what you have learnt in this course. This includes working with data and visualizing it, create model of linear regression or logistic regression, as well as using metrics to measure the accuracy of your model. \n",
    "\n",
    "Please find the project handout description in the following [link](https://sutdapac-my.sharepoint.com/:b:/g/personal/franklin_anariba_sutd_edu_sg/EaaE7XKJ0ZtJntc2IxEPiIYBVjijsZ5tUaPaH7mejSveWQ?e=1qADfd).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "You need to submit this Jupyter notebook together with the dataset into Vocareum. Use the template in this notebook to work on this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview About the Problem\n",
    "\n",
    "Describe here the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students Submission\n",
    "\n",
    "Student names and contributions:\n",
    "- Lee Jun Hui Ryan (1006652): Class implementation and evaluation\n",
    "- Yee Jia Zhen (1006969): Data cleaning & website\n",
    "- Lee Wei Jie (1006919): Data cleaning and video editing\n",
    "- Aishwarya Shivakumar Iyer (1007141): Visualization code & website \n",
    "- Dione Hannah Woon (1007071): Class implementation and code documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Data Cleaning\n",
    "\n",
    "Resources:\n",
    "| Dataset                                                      | Source                     |\n",
    "|--------------------------------------------------------------|----------------------------|\n",
    "|Cost of a healthy diet (PPP dollar per person per day) Dataset|https://www.fao.org/faostat/en/#data/CAHD  |\n",
    "Crude Birth Rates Dataset\t                                   |https://data.worldbank.org/indicator/SP.DYN.CBRT.IN |\n",
    "Food Price Inflation\t                                       |https://www.kaggle.com/datasets/anshtanwar/monthly-food-price-estimates |\n",
    "Global Peace Index\t                                           |https://www.kaggle.com/datasets/ddosad/global-peace-index-2023\n",
    "Global Terrorism Index                                         |https://www.kaggle.com/datasets/ddosad/global-terrorism-index-2023\n",
    "Happiness Index\t                                               |https://www.kaggle.com/datasets/anas123siddiqui/happiness-index-data\n",
    "Human Development Index\t                                       |https://www.kaggle.com/datasets/iamsouravbanerjee/human-development-index-dataset\n",
    "Poverty Headcount\t                                           |https://data.worldbank.org/indicator/SI.POV.NAHC?end=2022&start=2022&view=bar\n",
    "Agriculture Gross Production Index/ Meat Gross Production Index|https://data.worldbank.org/indicator/AG.PRD.FOOD.XD?end=2021&start=1961&view=chart\n",
    "World Freedom Index\t                                           |https://www.kaggle.com/datasets/sujaykapadnis/world-freedom-index\n",
    "Share of Employment in Agriculture\t                           |https://www.fao.org/faostat/en/#data/OEA\n",
    "World Population\t                                           |https://www.fao.org/faostat/en/#data/OA\n",
    "Others(QOL,PPI,SI,HCI,COLI.PPTIR,TCTI,PI,CI)                   |https://www.kaggle.com/datasets/ramjasmaurya/indexes-20122021\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#### Step 1: Data Formatting  \n",
    "We need to merge our datasets before we can use it for our multi linear regression model. Before we merge the datasets, we have to reformat them into our desired structure. Since most of our datasets are retrieved from FAOSTAT, the datasets share similar structures.  \n",
    "\n",
    "We identified 2 patterns in our datasets and we name them **Type 1** and **Type 2**.\n",
    "\n",
    "**Type 1**: \n",
    "- Country is represented by 'Country Name'.\n",
    "- The years are the column names of the dataset.  \n",
    "\n",
    "e.g.\n",
    "\n",
    "| Country Name | Indicator         | 2008 | 2009 | 2010 | 2011 |\n",
    "|--------------|-------------------|------|------|------|------|\n",
    "| Afghanistan  | name of indicator | a    | b    | c    | d    |\n",
    "| Bulgaria     | name of indicator | e    | f    | g    | h    |\n",
    "| Colombia     | name of indicator | i    | j    | k    | l    |\n",
    "\n",
    "<br>\n",
    "\n",
    "**Type 2**:\n",
    "- Country is represented by 'Country' or 'Area'.\n",
    "- The data is represented by 'Value'.\n",
    "- There is a 'Year' column for the years and the data is sorted in chronological order.\n",
    "\n",
    "e.g.\n",
    "\n",
    "| Country/Area | Indicator         | Year | Value |\n",
    "|--------------|-------------------|------|-------|\n",
    "| Afghanistan  | name of indicator | 2008 | a     |\n",
    "| Bulgaria     | name of indicator | 2008 | b     |\n",
    "| Afghanistan  | name of indicator | 2009 | c     |\n",
    "| Bulgaria     | name of indicator | 2009 | d     |\n",
    "| Afghanistan  | name of indicator | 2010 | e     |\n",
    "| Bulgaria     | name of indicator | 2010 | f     |\n",
    "\n",
    "<br>  \n",
    "\n",
    "**Type 3**:\n",
    "- Data is already in expected column (features), so we only need to remove rows with missing values via drop.\n",
    "- The format of type 3 datasets aka 'indexes by year' is in the following format:\n",
    "\n",
    "e.g. \n",
    "\n",
    "| Country | Year | Indicator Name | \n",
    "|---------|------|----------------|\n",
    "\n",
    "\n",
    "**Expected Output**\n",
    "\n",
    "Below is an illustration of how we would like to format our datasets. We want to keep the columns for the countries, years, and most importantly the values for their respective indicators. We will then create a new column with the indicator name as the column's name to store these values.  \n",
    "\n",
    "| Country | Year | Indicator Name | \n",
    "|---------|------|----------------|\n",
    "\n",
    "Since we have already decided on the year range (2017-2020), we will only extract data for these years.\n",
    "\n",
    "e.g.\n",
    "\n",
    "| Country     | Year | Human Development Index |  \n",
    "|-------------|------|-------------------------|\n",
    "| Afghanistan | 2017 | value1                  |\n",
    "| Afghanistan | 2018 | value2                  |\n",
    "| Afghanistan | 2019 | value3                  |\n",
    "| Afghanistan | 2020 | value4                  | \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def country_ls(df):\n",
    "    \"\"\"Get list of countries in dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    country_ls : list\n",
    "        List of countries from dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    country_ls = []\n",
    "\n",
    "    for country in df['Country']:\n",
    "        if country not in country_ls:\n",
    "            country_ls.append(country)\n",
    "    return country_ls\n",
    "\n",
    "\n",
    "def format_type1(csv, column_name:str, name='Country'):\n",
    "    \"\"\"Format Type 1 datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv : pd.DataFrame\n",
    "        Input DataFrame representing a Type 1 dataset.\n",
    "    column_name : str\n",
    "        Name of the indicator column.\n",
    "    name : str\n",
    "        Name of the column representing the country.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_final : pd.DataFrame \n",
    "        Formatted DataFrame with columns: 'Country', 'Year', and the specified indicator column.\n",
    "    \"\"\"\n",
    "    \n",
    "    valid_columns = ['Country Name', '2017', '2018', '2019', '2020']\n",
    "   \n",
    "    for y in csv.keys():\n",
    "        if y not in valid_columns:\n",
    "            csv = csv.drop([y], axis=1)\n",
    "\n",
    "    csv_T = csv.T\n",
    "    srs_ls = []\n",
    "    for index in range(csv_T.shape[1]):\n",
    "        srs_ls.append(csv_T[index])\n",
    "\n",
    "    df_ls = []\n",
    "    for i in range(len(srs_ls)):\n",
    "        srs = srs_ls[i]\n",
    "        df = srs.to_frame()\n",
    "        new_df = df.drop(index='Country Name', axis=1)\n",
    "        country = srs['Country Name']\n",
    "        new_df.insert(loc=0,\n",
    "              column= name,\n",
    "              value=country)\n",
    "        new_df.rename(columns={i: column_name}, inplace=True)\n",
    "        df_ls.append(new_df)\n",
    "    df_final = pd.concat(df_ls).reset_index(drop=False)\n",
    "\n",
    "    ls = country_ls(df_final)\n",
    "    for c in ls:\n",
    "        if df_final.loc[df_final['Country'] == c].shape[0] != 4: # 4 years\n",
    "            df_final = df_final.drop(df_final[df_final['Country'] == c].index)\n",
    "    print(type(df_final))\n",
    "    df_final.rename(columns={\"index\": \"Year\"}, inplace=True)\n",
    "\n",
    "    return df_final\n",
    "\n",
    "\n",
    "def format_type2(csv, column_name:str):\n",
    "    \"\"\"Format Type 2 datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv : pd.DataFrame \n",
    "        Input DataFrame representing a Type 2 dataset.\n",
    "    column_name : str \n",
    "        Name of the indicator column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    csv : pd.DataFrame\n",
    "        Formatted DataFrame with columns: 'Country', 'Year', and the specified indicator column.\n",
    "    \"\"\"\n",
    "\n",
    "    valid_columns = ['Area', 'Year', 'Value', 'Country']\n",
    "    valid_years = [2017, 2018, 2019, 2020, '2017', '2018', '2019', '2020']\n",
    "    \n",
    "    for key in csv.keys():\n",
    "        if key not in valid_columns:\n",
    "            csv = csv.drop([key], axis=1)\n",
    "\n",
    "    for yr in csv['Year']:\n",
    "        if yr not in valid_years:\n",
    "            csv = csv.drop(csv[csv['Year'] == yr].index)\n",
    "    \n",
    "    csv.rename(columns={\"Area\": \"Country\", \"Value\": column_name}, inplace=True)\n",
    "\n",
    "    ls = country_ls(csv)\n",
    "    for c in ls:\n",
    "        if csv.loc[csv['Country'] == c].shape[0] != 4: # 4 years\n",
    "            csv =csv.drop(csv[csv['Country'] == c].index)\n",
    "\n",
    "    return csv\n",
    "\n",
    "\n",
    "def format_type3(csv):\n",
    "    \"\"\"Format Type 3 datasets. ONLY for dataset 'indexes by year'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv : pd.DataFrame \n",
    "        Input DataFrame representing a 'indexes by year' dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    csv : pd.DataFrame\n",
    "        Formatted DataFrame with columns: 'Country', 'Year', and relevant indicator columns.\n",
    "    \"\"\"\n",
    "\n",
    "    valid_years = [2017, 2018, 2019, 2020, '2017', '2018', '2019', '2020']\n",
    "    for yr in csv['Year']:\n",
    "        if yr not in valid_years:\n",
    "            csv = csv.drop(csv[csv['Year'] == yr].index)\n",
    "\n",
    "    ls = country_ls(csv)\n",
    "    for c in ls:\n",
    "        if csv.loc[csv['Country'] == c].shape[0] != 4: # 4 years\n",
    "            csv =csv.drop(csv[csv['Country'] == c].index)\n",
    "    return csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Data Extraction\n",
    "\n",
    "Every country should have data for each of the features that we have identified. Since the list of countries in each dataset is not the same, we cannot compare the data and would have to remove them before merging. This is to make sure every country has data for each of the features for all the datasets that we will be using for the model.\n",
    "\n",
    "We wrote 2 functions for this task. \n",
    "1. `common_countries()`:\\\n",
    "This function returns a list of countries that can be found in all the datasets.\\\n",
    "Input: list of datasets that we will be using\\\n",
    "Output: list of common countries\n",
    "\n",
    "2. `rm_irrelevant_countries()`\\\n",
    "This function uses the `common_countries()` function to remove the common countries in all the datasets.\\\n",
    "Input: list of datasets to clean\\\n",
    "Output: list of cleaned dataset (i.e. only data for common countries remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_countries(dfs:list):\n",
    "    \"\"\"Find common countries among multiple datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list\n",
    "        List of DataFrames to find common countries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list \n",
    "        Sorted list of common countries found in all input DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    common_ls = []\n",
    "    for df in dfs:\n",
    "        common_ls.append(country_ls(df))\n",
    "\n",
    "    common_elements = set(common_ls[0])\n",
    "    for lst in common_ls[1:]:\n",
    "        common_elements = common_elements & set(lst)\n",
    "\n",
    "    return sorted(list(common_elements))\n",
    "\n",
    "\n",
    "def rm_irrelevant_countries(dfs: list, country_keep: list):\n",
    "    \"\"\"Remove irrelevant countries from datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list \n",
    "        List of DataFrames to be cleaned.\n",
    "    country_keep : list\n",
    "        List of countries to keep in the datasets.\n",
    "\n",
    "    Returns:\n",
    "    final_ls : list\n",
    "        List of cleaned DataFrames with only relevant countries.\n",
    "    \"\"\"\n",
    "        \n",
    "    final_ls = []\n",
    "    for csv in dfs:\n",
    "        relevant_rows = csv['Country'].isin(country_keep)\n",
    "        csv = csv[relevant_rows]\n",
    "        final_ls.append(csv)\n",
    "        \n",
    "    return final_ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Merging\n",
    "\n",
    "Now that the datasets are cleaned, all of them should have data of the same list of countries. The next step is to merge the datasets and remove all rows containing null values.\n",
    "\n",
    "This can be done using `sort_dataframes_by_country()`.\\\n",
    "Input: list of dataframes to merge\\\n",
    "Output: merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dataframes_by_country(dfs: list):\n",
    "    \"\"\"Sort and merge cleaned datasets by the 'Country' column, in alphabetical order.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list\n",
    "        List of cleaned DataFrames to be sorted and merged.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean_merged : pd.DataFrame\n",
    "        Merged DataFrame with sorted rows based on the 'Country' column, in alphabetical order.\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_dfs = []\n",
    "    for csv in dfs:\n",
    "        sorted_df = csv.sort_values(by='Country')\n",
    "        sorted_dfs.append(sorted_df.reset_index(drop=True))\n",
    "\n",
    "    merged = pd.concat(sorted_dfs, axis=1 )\n",
    "    clean_merged = merged.loc[:, ~merged.columns.duplicated(keep='first')]\n",
    "    \n",
    "    return clean_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: type2 -> datasets/Cost of a healthy diet (PPP dollar per person per day).csv\n",
      "1: type2 -> datasets/WorldPopulation - Employment.csv\n",
      "2: type2 -> datasets/World Freedom Index .csv\n",
      "3: type2 -> datasets/total_agriculture_production.csv\n",
      "4: type2 -> datasets/total_meat_production.csv\n",
      "5: type3 -> datasets/Indexes by Year.csv\n",
      "6: type1 -> datasets/Poverty headcount.csv\n",
      "7: type1 -> datasets/Human Development Index - Full.csv\n",
      "8: type1 -> datasets/Global Terrorism Index 2023.csv\n",
      "9: type2 -> datasets/Global Peace Index .csv\n",
      "10: type2 -> datasets/Food Price Inflation.csv\n",
      "11: type1 -> datasets/Crude BirthRates.csv\n",
      "12: type2 -> datasets/WorldPopulation.csv\n",
      "13: type1 -> datasets/happiness dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Read CSV and identify their types\n",
    "\n",
    "\n",
    "# Set the path to the directory containing your CSV files\n",
    "csv_files_path = 'datasets/*.csv'\n",
    "\n",
    "# Get a list of all CSV files in the specified directory\n",
    "csv_files = glob.glob(csv_files_path)\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for i, file_path in enumerate(csv_files):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    if 'Value' in df.keys():\n",
    "        print(f'{i}: type2 -> {file_path}')\n",
    "    elif '2017' in df.keys():\n",
    "        print(f'{i}: type1 -> {file_path}')\n",
    "    else:\n",
    "        print(f'{i}: type3 -> {file_path}')\n",
    "        # print(df.keys())\n",
    "\n",
    "    # Store the DataFrame in the dictionary with the variable name as the key\n",
    "    dataframes[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Data Formatting\n",
    "\n",
    "df0 = format_type2(dataframes[0], 'Cost of a healthy diet (PPP dollar per person per day)')\n",
    "df1 = format_type2(dataframes[1].loc[dataframes[1]['Indicator'].isin(['Employment by status of employment, total, rural areas'])], 'Employment')\n",
    "df2 = format_type2(dataframes[2], 'World Freedom Index')\n",
    "df3 = format_type2(dataframes[3], 'Agriculture Gross Production Index')\n",
    "df4 = format_type2(dataframes[4], 'Meat Gross Production Index')\n",
    "df5 = format_type3(dataframes[5])\n",
    "df6 = format_type1(dataframes[6], 'Poverty Headcount')\n",
    "df7 = format_type1(dataframes[7], 'Human Development Index')\n",
    "df8 = format_type1(dataframes[8], 'Global Terrorism Index')\n",
    "df9 = format_type2(dataframes[9], 'Global Peace Index')\n",
    "df10 = format_type2(dataframes[10], 'Food Price Inflation')\n",
    "df11 = format_type1(dataframes[11], 'Crude Birth Rates')\n",
    "df12 = format_type2(dataframes[12], 'World Population')\n",
    "df13 = format_type1(dataframes[13], 'Happiness Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country                                                    0\n",
      "Year                                                       0\n",
      "Cost of a healthy diet (PPP dollar per person per day)     0\n",
      "Employment                                                 0\n",
      "World Freedom Index                                        0\n",
      "Agriculture Gross Production Index                         0\n",
      "Meat Gross Production Index                                0\n",
      "Quality of Life Index                                      0\n",
      "Purchasing Power Index                                     0\n",
      "Safety Index                                               0\n",
      "Health Care Index                                          0\n",
      "Cost of Living Index                                       0\n",
      "Property Price to Income Ratio                             0\n",
      "Traffic Commute Time Index                                 0\n",
      "Pollution Index                                            0\n",
      "Climate Index                                              0\n",
      "Poverty Headcount                                         32\n",
      "Human Development Index                                    0\n",
      "Global Terrorism Index                                     0\n",
      "Global Peace Index                                         0\n",
      "Food Price Inflation                                       0\n",
      "Crude Birth Rates                                          0\n",
      "World Population                                           0\n",
      "Happiness Index                                            0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Cost of a healthy diet (PPP dollar per person per day)</th>\n",
       "      <th>Employment</th>\n",
       "      <th>World Freedom Index</th>\n",
       "      <th>Agriculture Gross Production Index</th>\n",
       "      <th>Meat Gross Production Index</th>\n",
       "      <th>Quality of Life Index</th>\n",
       "      <th>Purchasing Power Index</th>\n",
       "      <th>Safety Index</th>\n",
       "      <th>...</th>\n",
       "      <th>Pollution Index</th>\n",
       "      <th>Climate Index</th>\n",
       "      <th>Poverty Headcount</th>\n",
       "      <th>Human Development Index</th>\n",
       "      <th>Global Terrorism Index</th>\n",
       "      <th>Global Peace Index</th>\n",
       "      <th>Food Price Inflation</th>\n",
       "      <th>Crude Birth Rates</th>\n",
       "      <th>World Population</th>\n",
       "      <th>Happiness Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2017</td>\n",
       "      <td>2.772</td>\n",
       "      <td>1695.65</td>\n",
       "      <td>95</td>\n",
       "      <td>98.48</td>\n",
       "      <td>98.58</td>\n",
       "      <td>190.37</td>\n",
       "      <td>95.66</td>\n",
       "      <td>80.75</td>\n",
       "      <td>...</td>\n",
       "      <td>21.90</td>\n",
       "      <td>62.13</td>\n",
       "      <td>14.3</td>\n",
       "      <td>0.916</td>\n",
       "      <td>2.564</td>\n",
       "      <td>1.309</td>\n",
       "      <td>3.168317</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8797496</td>\n",
       "      <td>7.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.848</td>\n",
       "      <td>1705.07</td>\n",
       "      <td>94</td>\n",
       "      <td>100.47</td>\n",
       "      <td>96.98</td>\n",
       "      <td>182.50</td>\n",
       "      <td>82.38</td>\n",
       "      <td>76.27</td>\n",
       "      <td>...</td>\n",
       "      <td>22.19</td>\n",
       "      <td>77.3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0.917</td>\n",
       "      <td>2.342</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.863724</td>\n",
       "      <td>9.7</td>\n",
       "      <td>8840513</td>\n",
       "      <td>7.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.915</td>\n",
       "      <td>1719.75</td>\n",
       "      <td>93</td>\n",
       "      <td>99.94</td>\n",
       "      <td>96.90</td>\n",
       "      <td>190.22</td>\n",
       "      <td>98.69</td>\n",
       "      <td>79.59</td>\n",
       "      <td>...</td>\n",
       "      <td>22.87</td>\n",
       "      <td>77.74</td>\n",
       "      <td>13.9</td>\n",
       "      <td>0.919</td>\n",
       "      <td>1.798</td>\n",
       "      <td>1.259</td>\n",
       "      <td>0.475737</td>\n",
       "      <td>9.6</td>\n",
       "      <td>8879940</td>\n",
       "      <td>7.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Austria</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.004</td>\n",
       "      <td>1715.81</td>\n",
       "      <td>93</td>\n",
       "      <td>101.82</td>\n",
       "      <td>95.66</td>\n",
       "      <td>191.05</td>\n",
       "      <td>96.70</td>\n",
       "      <td>78.63</td>\n",
       "      <td>...</td>\n",
       "      <td>21.97</td>\n",
       "      <td>77.74</td>\n",
       "      <td>14.7</td>\n",
       "      <td>0.913</td>\n",
       "      <td>3.803</td>\n",
       "      <td>1.265</td>\n",
       "      <td>2.775852</td>\n",
       "      <td>9.4</td>\n",
       "      <td>8907777</td>\n",
       "      <td>7.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.962</td>\n",
       "      <td>705.03</td>\n",
       "      <td>95</td>\n",
       "      <td>98.28</td>\n",
       "      <td>105.04</td>\n",
       "      <td>164.00</td>\n",
       "      <td>98.91</td>\n",
       "      <td>57.83</td>\n",
       "      <td>...</td>\n",
       "      <td>48.92</td>\n",
       "      <td>86.14</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.933</td>\n",
       "      <td>4.779</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.880804</td>\n",
       "      <td>10.4</td>\n",
       "      <td>11448595</td>\n",
       "      <td>6.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.639</td>\n",
       "      <td>789.71</td>\n",
       "      <td>96</td>\n",
       "      <td>99.03</td>\n",
       "      <td>100.42</td>\n",
       "      <td>190.81</td>\n",
       "      <td>126.15</td>\n",
       "      <td>78.24</td>\n",
       "      <td>...</td>\n",
       "      <td>23.01</td>\n",
       "      <td>79.78</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.959</td>\n",
       "      <td>3.094</td>\n",
       "      <td>1.365</td>\n",
       "      <td>0.378575</td>\n",
       "      <td>10.3</td>\n",
       "      <td>8638613</td>\n",
       "      <td>7.494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>2017</td>\n",
       "      <td>3.971</td>\n",
       "      <td>20396.20</td>\n",
       "      <td>32</td>\n",
       "      <td>102.33</td>\n",
       "      <td>99.80</td>\n",
       "      <td>101.18</td>\n",
       "      <td>38.59</td>\n",
       "      <td>52.75</td>\n",
       "      <td>...</td>\n",
       "      <td>72.86</td>\n",
       "      <td>68.64</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6.901</td>\n",
       "      <td>2.189</td>\n",
       "      <td>0.025742</td>\n",
       "      <td>9.98</td>\n",
       "      <td>70898202</td>\n",
       "      <td>5.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>2018</td>\n",
       "      <td>4.042</td>\n",
       "      <td>20659.06</td>\n",
       "      <td>31</td>\n",
       "      <td>105.29</td>\n",
       "      <td>100.52</td>\n",
       "      <td>103.26</td>\n",
       "      <td>40.86</td>\n",
       "      <td>53.34</td>\n",
       "      <td>...</td>\n",
       "      <td>72.21</td>\n",
       "      <td>69.45</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.795</td>\n",
       "      <td>6.662</td>\n",
       "      <td>2.147</td>\n",
       "      <td>1.039929</td>\n",
       "      <td>9.662</td>\n",
       "      <td>71127802</td>\n",
       "      <td>6.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>2019</td>\n",
       "      <td>4.181</td>\n",
       "      <td>20443.25</td>\n",
       "      <td>30</td>\n",
       "      <td>102.26</td>\n",
       "      <td>101.42</td>\n",
       "      <td>101.88</td>\n",
       "      <td>35.45</td>\n",
       "      <td>59.52</td>\n",
       "      <td>...</td>\n",
       "      <td>75.07</td>\n",
       "      <td>69.45</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.804</td>\n",
       "      <td>6.567</td>\n",
       "      <td>2.181</td>\n",
       "      <td>1.505000</td>\n",
       "      <td>9.377</td>\n",
       "      <td>71307763</td>\n",
       "      <td>6.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>2020</td>\n",
       "      <td>4.321</td>\n",
       "      <td>20500.42</td>\n",
       "      <td>32</td>\n",
       "      <td>96.76</td>\n",
       "      <td>99.78</td>\n",
       "      <td>57.21</td>\n",
       "      <td>34.28</td>\n",
       "      <td>50.22</td>\n",
       "      <td>...</td>\n",
       "      <td>73.23</td>\n",
       "      <td>-20.21</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.802</td>\n",
       "      <td>6.179</td>\n",
       "      <td>2.178</td>\n",
       "      <td>1.702698</td>\n",
       "      <td>9.182</td>\n",
       "      <td>71475664</td>\n",
       "      <td>6.008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Country  Year  \\\n",
       "4        Austria  2017   \n",
       "5        Austria  2018   \n",
       "6        Austria  2019   \n",
       "7        Austria  2020   \n",
       "9        Belgium  2018   \n",
       "..           ...   ...   \n",
       "119  Switzerland  2020   \n",
       "120     Thailand  2017   \n",
       "121     Thailand  2018   \n",
       "122     Thailand  2019   \n",
       "123     Thailand  2020   \n",
       "\n",
       "     Cost of a healthy diet (PPP dollar per person per day)  Employment  \\\n",
       "4                                                2.772          1695.65   \n",
       "5                                                2.848          1705.07   \n",
       "6                                                2.915          1719.75   \n",
       "7                                                3.004          1715.81   \n",
       "9                                                2.962           705.03   \n",
       "..                                                 ...              ...   \n",
       "119                                              2.639           789.71   \n",
       "120                                              3.971         20396.20   \n",
       "121                                              4.042         20659.06   \n",
       "122                                              4.181         20443.25   \n",
       "123                                              4.321         20500.42   \n",
       "\n",
       "    World Freedom Index  Agriculture Gross Production Index  \\\n",
       "4                    95                               98.48   \n",
       "5                    94                              100.47   \n",
       "6                    93                               99.94   \n",
       "7                    93                              101.82   \n",
       "9                    95                               98.28   \n",
       "..                  ...                                 ...   \n",
       "119                  96                               99.03   \n",
       "120                  32                              102.33   \n",
       "121                  31                              105.29   \n",
       "122                  30                              102.26   \n",
       "123                  32                               96.76   \n",
       "\n",
       "     Meat Gross Production Index  Quality of Life Index  \\\n",
       "4                          98.58                 190.37   \n",
       "5                          96.98                 182.50   \n",
       "6                          96.90                 190.22   \n",
       "7                          95.66                 191.05   \n",
       "9                         105.04                 164.00   \n",
       "..                           ...                    ...   \n",
       "119                       100.42                 190.81   \n",
       "120                        99.80                 101.18   \n",
       "121                       100.52                 103.26   \n",
       "122                       101.42                 101.88   \n",
       "123                        99.78                  57.21   \n",
       "\n",
       "     Purchasing Power Index  Safety Index  ...  Pollution Index  \\\n",
       "4                     95.66         80.75  ...            21.90   \n",
       "5                     82.38         76.27  ...            22.19   \n",
       "6                     98.69         79.59  ...            22.87   \n",
       "7                     96.70         78.63  ...            21.97   \n",
       "9                     98.91         57.83  ...            48.92   \n",
       "..                      ...           ...  ...              ...   \n",
       "119                  126.15         78.24  ...            23.01   \n",
       "120                   38.59         52.75  ...            72.86   \n",
       "121                   40.86         53.34  ...            72.21   \n",
       "122                   35.45         59.52  ...            75.07   \n",
       "123                   34.28         50.22  ...            73.23   \n",
       "\n",
       "     Climate Index  Poverty Headcount  Human Development Index  \\\n",
       "4            62.13               14.3                    0.916   \n",
       "5             77.3               13.3                    0.917   \n",
       "6            77.74               13.9                    0.919   \n",
       "7            77.74               14.7                    0.913   \n",
       "9            86.14               14.8                    0.933   \n",
       "..             ...                ...                      ...   \n",
       "119          79.78               16.0                    0.959   \n",
       "120          68.64                7.9                     0.79   \n",
       "121          69.45                9.9                    0.795   \n",
       "122          69.45                6.2                    0.804   \n",
       "123         -20.21                6.8                    0.802   \n",
       "\n",
       "     Global Terrorism Index Global Peace Index Food Price Inflation  \\\n",
       "4                     2.564              1.309             3.168317   \n",
       "5                     2.342              1.272             0.863724   \n",
       "6                     1.798              1.259             0.475737   \n",
       "7                     3.803              1.265             2.775852   \n",
       "9                     4.779               1.55             1.880804   \n",
       "..                      ...                ...                  ...   \n",
       "119                   3.094              1.365             0.378575   \n",
       "120                   6.901              2.189             0.025742   \n",
       "121                   6.662              2.147             1.039929   \n",
       "122                   6.567              2.181             1.505000   \n",
       "123                   6.179              2.178             1.702698   \n",
       "\n",
       "    Crude Birth Rates World Population Happiness Index  \n",
       "4                10.0          8797496           7.006  \n",
       "5                 9.7          8840513           7.294  \n",
       "6                 9.6          8879940           7.139  \n",
       "7                 9.4          8907777           7.246  \n",
       "9                10.4         11448595           6.923  \n",
       "..                ...              ...             ...  \n",
       "119              10.3          8638613           7.494  \n",
       "120              9.98         70898202           5.999  \n",
       "121             9.662         71127802           6.424  \n",
       "122             9.377         71307763           6.072  \n",
       "123             9.182         71475664           6.008  \n",
       "\n",
       "[92 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please put all datasets into a list called df_ls\n",
    "df_ls = [df0, df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13]\n",
    "ck = common_countries(df_ls)\n",
    "df_final = rm_irrelevant_countries(df_ls, ck)\n",
    "df = sort_dataframes_by_country(df_final)\n",
    "\n",
    "# For checking purposes\n",
    "print(df.isnull().sum())\n",
    "df = df.dropna() # drop all rows with missing data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "\n",
    "df.to_csv('merged_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Target Preparation\n",
    "\n",
    "*Describe here what are the features you use and why these features. Put any Python codes to prepare and clean up your features. Do the same thing for the target. Describe your target and put any codes to prepare your target.*\n",
    "\n",
    "In this section, we will be visualizing our data and deciding which features to use for our model. Our target is \"Cost of a healthy diet (PPP dollar per person per day)\" in order for our model to address our problem statement. The data also needs to be prepared in other ways (e.g. adding intercept term, normalization, type conversions, etc) will be carried out by our multiple linear regression model (implemented as a class). Please refer to the section \"Building Model\" for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Employment', 'World Freedom Index', 'Agriculture Gross Production Index', 'Meat Gross Production Index', 'Quality of Life Index', 'Purchasing Power Index', 'Safety Index', 'Health Care Index', 'Cost of Living Index', 'Property Price to Income Ratio', 'Traffic Commute Time Index', 'Pollution Index', 'Climate Index', 'Poverty Headcount', 'Human Development Index', 'Global Terrorism Index', 'Global Peace Index', 'Food Price Inflation', 'Crude Birth Rates', 'World Population', 'Happiness Index']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"merged_dataset.csv\") # Load in cleaned data\n",
    "target = \"Cost of a healthy diet (PPP dollar per person per day)\" # Chosen target\n",
    "features = list(df.columns) # all variables in DataFrame\n",
    "\n",
    "# Remove non-features:\n",
    "features.remove(target)\n",
    "features.remove(\"Year\")\n",
    "features.remove(\"Country\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our data by plotting each feature against our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plots (everything should be 1-dimensional)\n",
    "num_features = len(features)\n",
    "y = df[target] # Series\n",
    "\n",
    "# Calculate the number of rows and columns for subplots (n plots per row)\n",
    "n, size = 3, 5 # configuration parameters for visualization\n",
    "num_rows = int(np.ceil(num_features / n))\n",
    "num_cols = min(num_features, n)\n",
    "width = size * num_cols\n",
    "height = size * num_rows\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(width, height))\n",
    "if num_features > 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes]\n",
    "\n",
    "features_r2 = [] # store features and their corresponding r2 values\n",
    "\n",
    "# Plot target against each feature\n",
    "for i in range(num_features):\n",
    "    feature = features[i]\n",
    "    x = df[feature] # Series\n",
    "    r = np.corrcoef(x, y)[0, 1] # correlation coefficient\n",
    "    r2 = np.square(r)\n",
    "    features_r2.append((feature, r2))\n",
    "    \n",
    "    sns.scatterplot(x=x, y=y, ax=axes[i], color=\"blue\", marker=\"o\", label=f\"r2 = {r2:.5f}\")\n",
    "\n",
    "    # Add labels and title\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel(target)\n",
    "    axes[i].set_title(\"  \")\n",
    "    # axes[i].set_title(f\"Scatter plot of {target} against {feature}\")\n",
    "    axes[i].legend()\n",
    "\n",
    "# Hide remaining empty subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we have plotted out scatter plots of our target variable against each feature and calculated the respective r2 values. The closer r2 is a value between 0 and 1; the larger the r2 value, the stronger the correlation between the variables. We have stored the features and their r2 values in a list called 'features_r2'.<br>For multiple linear regression, we want to limit the number of features we use and only keep the most relevant features. This is because we have limited data points and we want to avoid the 'curse of dimensionality'. Having 8-10 times more data points than features should be good enough. As such, we will only be using 8 of the most relevant features, where relevance is decided based on the r2 value of the feature and target, which indicates the strength of their correlation. We will sort the features_r2 list in descending order of r2 value and use the first 8 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort features by their r2 value in descending order:\n",
    "features_r2.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "# Display the 8 features with the largest r2 (strongest correlation with target):\n",
    "chosen_features = [] \n",
    "for i in range(8):\n",
    "    feature = features_r2[i][0]\n",
    "    r2 = features_r2[i][1]\n",
    "    print(f\"{feature} : {r2:.5f}\")\n",
    "    chosen_features.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our chosen target and features have been stored in the 'target' and 'chosen_features' variables respectively. Of course, for multiple linear regression, this data will need to be prepared properly (e.g. adding intercept term, normalization, storing in NumPy arrays, etc) to be used by training and prediction algorithms. For this project, we have decided to use a class based approach where these preparations are performed by the model - the user only needs to prepare the appropriate pandas DataFrame. More details on this in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Model\n",
    "\n",
    "*Describe your model. Is this Linear Regression or Logistic Regression? Put any other details about the model. Put the codes to build your model.*\n",
    "\n",
    "We implemented multiple linear regression using a class, in which an instance represents an individual multiple linear regression model. We used a class for modularity and reusability, allowing us to easily use the model in different parts of the code and in other projects. Additionally, a class provides a layer of abstraction that allows users of the class to work with the model without understanding the internal workings and implementation details. Furthermore, because things like parameter values are stored in the instance itself, it is easy to experiment with models that have different configurations. Finally, using a class allows us to easily implement variations of the model using inheritance (as we will demonstrate later).\n",
    "\n",
    "We have designed this class such that each model can be trained to predict multiple target variables with the same input features. In other words, the model stores multiple sets of parameter values for the prediction of different target variables. Additionally, as mentioned in the 'Features and Target Preparation' section, our class handles data preparation (type conversion, intercept term, normalization, etc) for the user. Please see the class and method documentation below for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleLinearRegression:\n",
    "    \"\"\"Class for a multiple linear regression model.\n",
    "\n",
    "    Extended Summary\n",
    "    ----------------\n",
    "    The model is designed to be trained to predict multiple target variables using\n",
    "    the same input features Hence, the model stores separate sets of parameter values\n",
    "    for the prediction of each target variable.\n",
    "    General user flow:\n",
    "    1) Store data (expects pandas DataFrames) using store_data().\n",
    "    2) Train the model to predict the specified target variable using train().\n",
    "    3) Validate the model's performance using validate().\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    params : dict\n",
    "        Dictionary that stores the different sets of parameter values for different\n",
    "        target variables. The key is the name of a target variable (str) while\n",
    "        the value is a numpy.ndarray containing the parameter values. New key-value\n",
    "        pairs (i.e. new parameters) are added or updated when the model is trained to\n",
    "        predict a specified target variable. Each numpy.ndarray (set of parameter values)\n",
    "        has a shape of (n+1,1) where n is the number of features.\n",
    "    features, targets : list of str\n",
    "        A list of strings containing the column names of the features and targets respectively.\n",
    "        Initialised when data is stored in the instance using store_data().\n",
    "    train_features, test_features, train_targets, test_targets : pandas.DataFrames\n",
    "        Pandas Dataframes containing the features/targets for training/testing. Initialised\n",
    "        when data is stored in the instance using store_data().\n",
    "    means, stds : float\n",
    "        The mean and standard deviation of the training features, initialised during training.\n",
    "        Used for normalization of input features for model predictions.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    store_data(data, feature_names, target_names, random_state=None, test_size=0.5)\n",
    "        Takes the given data and separates the features and targets. Then splits the data\n",
    "        for training and testing before storing it in the model's attributes.\n",
    "    add_transformed_feature(feature_name, new_feature_name, transform_func, replace=False)\n",
    "        Transforms a specified feature and adds it to the stored data.\n",
    "    train(target, alpha=0.01, epochs=1000, retrain=False, convergence_threshold=1e-6, show=True)\n",
    "        Trains the model to predict the specified target variable with the input features. Stores\n",
    "        the parameters in the model.\n",
    "    predict(df_features, target)\n",
    "        Given some input data of features, the model returns its predictions for the specified target\n",
    "        variable using linear regression (with its stored parameters).\n",
    "    validate(target, visualize=True)\n",
    "        Evaluate the model's ability to predict the specified target variable (its performance) using the\n",
    "        test data. Will show visualizations of the predicted values against the true values for the test data\n",
    "        and return various performance metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructs a new instance of MultipleLinearRegression\"\"\"\n",
    "\n",
    "        self.params = {} # dictionary of weights (numpy arrays)\n",
    "        self.features, self.targets = None, None # list of feature/target column names\n",
    "        self.train_features, self.train_targets = None, None\n",
    "        self.test_features, self.test_targets = None, None\n",
    "        self.means, self.stds = None, None # for normalization (initialised during training)\n",
    "\n",
    "\n",
    "    def store_data(self, data, feature_names, target_names, random_state=None, test_size=0.5):\n",
    "        \"\"\"Prepares and stores data in the model (instance).\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Will split the dataset into two: one portion for training and the other for testing (model validation).\n",
    "        The features and targets for training/testing are stored in the instance as attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas.DataFrame\n",
    "            DataFrame that contains all the relevant data (features and targets).\n",
    "        feature_names, target_names : list\n",
    "            Lists containing the column names for the features and targets respectively.\n",
    "        random_state : int, optional\n",
    "            Integer representing a seed for splitting of the data into training and test sets.\n",
    "            Setting a seed to a specific value ensures reproducibility (default is None, which means\n",
    "            that no seed will be specified and data splitting will be random).\n",
    "        test_size : float, default 0.5\n",
    "            Float value between 0 and 1 that represents the fraction of the data that will be in the test set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Split data into features and targets\n",
    "        self.features = feature_names.copy()\n",
    "        self.targets = target_names.copy()\n",
    "        df_features = data.loc[:, feature_names]\n",
    "        df_targets = data.loc[:, target_names]\n",
    "\n",
    "        # Split data into training and testing sets, then store in instance\n",
    "        self.train_features, self.test_features, self.train_targets, self.test_targets = self._split_data(df_features, df_targets, random_state, test_size)\n",
    "\n",
    "\n",
    "    def add_transformed_feature(self, feature_name, new_feature_name, transform_func, replace=False):\n",
    "        \"\"\"Takes a feature in the stored data and stores a transformed version of it.\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Transforms the specified feature (already stored in the model) by applying the transformation\n",
    "        function on each feature value. The new transformed feature values can be stored as a new\n",
    "        feature or can replace the original feature which was transformed. Important to note: the model\n",
    "        should be retrained after adding a transformed feature.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_name : str\n",
    "            Column name of the feature to be transformed. The feature should exist in the model's\n",
    "            stored data.\n",
    "        new_feature_name : str\n",
    "            The column name of the transformed feature to be added.\n",
    "        transform_func:\n",
    "            The name of the transformation function i.e. the function to be applied to the specified feature\n",
    "            values to transform them.\n",
    "        replace : bool, default False\n",
    "            Determines whether transformed features will replace the original feature or be added as a separate\n",
    "            feature. (Default is False, so the transformed features will be added as a new column with the\n",
    "            column name specified with parameter new_feature_name).\n",
    "        \"\"\"\n",
    "\n",
    "        if feature_name in self.features:\n",
    "            # add new column of transformed features to both training and test features\n",
    "            train_features_transformed = self.train_features[feature_name].apply(transform_func)\n",
    "            test_features_transformed = self.test_features[feature_name].apply(transform_func)\n",
    "\n",
    "            if replace == True:\n",
    "                # Replace data:\n",
    "                self.train_features[feature_name] = train_features_transformed\n",
    "                self.test_features[feature_name] = test_features_transformed\n",
    "\n",
    "                # Rename:\n",
    "                self.train_features.rename(columns={feature_name: new_feature_name}, inplace=True)\n",
    "                self.test_features.rename(columns={feature_name: new_feature_name}, inplace=True)\n",
    "                self.features = [new_feature_name if item == feature_name else item for item in self.features]\n",
    "\n",
    "            else:\n",
    "                # Add as new column\n",
    "                self.train_features[new_feature_name] = train_features_transformed\n",
    "                self.test_features[new_feature_name] = test_features_transformed\n",
    "                self.features.append(new_feature_name)\n",
    "        else:\n",
    "            raise ValueError(f\"No feature called {feature_name} in stored data.\")\n",
    "\n",
    "\n",
    "    def train(self, target, alpha=0.01, epochs=1000, retrain=False, convergence_threshold=1e-6, show=True):\n",
    "        \"\"\"Train the model to predict the specified target variable (using the stored training data).\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Performs batch gradient descent with training data stored using the store_data method.\n",
    "        Updates the 'param' attribute with new parameter values. Cost is defined as half the mean squared error.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : str\n",
    "            Column name of target variable to be predicted.\n",
    "        alpha : float, default 0.01\n",
    "            Learning rate for gradient descent.\n",
    "        epochs : int, default 1000\n",
    "            Number of gradient descent steps.\n",
    "        retrain : bool, default False\n",
    "            If True, reinitialize the model parameters. (Default is False, meaning\n",
    "            training will start using current parameter values if they exist)\n",
    "        convergence_threshold : float, default 1e-6\n",
    "            Threshold for convergence based on the change in cost. Training will end\n",
    "            prematurely if difference in cost between iterations is less than the threshold.\n",
    "        show : bool, default True\n",
    "            If True, will show change in cost during training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self.params : numpy.ndarray\n",
    "            NumPy array containing parameter values.\n",
    "        J_storage : numpy.ndarray\n",
    "            NumPy array containing the cost after each iteration of gradient descent.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check that there is data stored for training and validation:\n",
    "        if any(data is None for data in (self.train_features, self.train_targets, self.test_features, self.test_targets)):\n",
    "            raise ValueError(\"Data missing. Run the store_data() method first.\")\n",
    "        # Check that valid target name is specified:\n",
    "        elif target not in self.targets:\n",
    "            raise ValueError(\"Specified target not found in stored data.\")\n",
    "\n",
    "\n",
    "        # Normalize training features:\n",
    "        train_features_norm, self.means, self.stds = self._normalize_z(self.train_features)\n",
    "\n",
    "        # Prepare data in numpy arrays:\n",
    "        X_train = self._prepare_X(train_features_norm)\n",
    "        y_train = self._prepare_y(self.train_targets.loc[:, [target]])\n",
    "        m, n = X_train.shape  # m = number of training examples, n = number of features (including intercept term)\n",
    "\n",
    "        # Validate parameters\n",
    "        params = self.params.setdefault(target, np.zeros((n,1)) ) # initialize if not exist\n",
    "        if retrain or params.shape[0] != n: # check if dimensions match\n",
    "            self.params[target] = np.zeros((n,1)) # reset\n",
    "\n",
    "        # Gradient Descent\n",
    "        J_storage = np.zeros(epochs)\n",
    "        if show: print(\"TRAINING START\")\n",
    "        for i in range(epochs):\n",
    "            y_hat = self._calc_linreg(X_train, self.params[target])\n",
    "            err = y_hat - y_train\n",
    "\n",
    "            # Measure cost J in each iteration (epoch):\n",
    "            J = np.matmul(err.T, err) / (2*m)\n",
    "            J_storage[i] = J[0][0] # scalar value of (1,1) array\n",
    "\n",
    "            # Update parameters (according to partial derivatives of cost function J):\n",
    "            deriv = np.matmul(X_train.T, err) / m\n",
    "            self.params[target] = self.params[target] - alpha * deriv  # update stored parameters\n",
    "\n",
    "            if show: print(f\"Epoch {i + 1:<4} || Cost: {J[0][0]:.5f}\")\n",
    "            # Check for convergence:\n",
    "            if i > 0 and abs(J_storage[i] - J_storage[i-1]) < convergence_threshold:\n",
    "                print(f\"Converged (threshold = {convergence_threshold}). Stopping training.\")\n",
    "                break\n",
    "\n",
    "        if show:\n",
    "            print(\"TRAINING END\")\n",
    "            plt.figure(figsize=(5, 3)) # size of plot\n",
    "            sns.lineplot(x=range(1, len(J_storage) + 1), y=J_storage)\n",
    "            plt.xlabel(\"Epochs\")\n",
    "            plt.ylabel(\"Cost (J)\")\n",
    "            plt.title(\"Training Cost Over Time\")\n",
    "            plt.show()\n",
    "\n",
    "        return self.params, J_storage\n",
    "\n",
    "\n",
    "    def predict(self, df_features, target):\n",
    "        \"\"\"Model returns predictions of specified target using the given input features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_features : pandas.DataFrame\n",
    "            Data containing input features to predict with, where each row is a set of input features (x).\n",
    "            It is a DataFrame with shape (m,n), where m is the number of data points to be predicted\n",
    "            and n is the number of feature variables.\n",
    "        target : str\n",
    "            Column name of target variable to be predicted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_hat : numpy.ndarray\n",
    "            NumPy array that has a shape of (m,1) and contains the model's predictions for each\n",
    "            of the input data points.\n",
    "        \"\"\"\n",
    "\n",
    "        self._target_valid(target)\n",
    "        norm_features,_,_ = self._normalize_z(df_features, self.means, self.stds) # normalize input features using training feature means and stds\n",
    "        X = self._prepare_X(norm_features)\n",
    "        y_hat = self._calc_linreg(X, self.params[target])\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def validate(self, target, visualize=True):\n",
    "        \"\"\"Evaluate the model's performance using testing data and return performance metrics.\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Using the testing data, the model will be evaluated by comparing its predictions (of the\n",
    "        specified target variable) with the true target values. This comparison is visualised with\n",
    "        scatter plots of the target variable against each input feature (optional). The following metrics,\n",
    "        which evaluate the multiple linear regression model's predictions, are computed and returned:\n",
    "        - Mean Square Error\n",
    "        - Mean Absolute Error\n",
    "        - R2 coefficient of determination\n",
    "        - Adjusted R2 coefficient of determination\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : str\n",
    "            Column name of target variable to be predicted. The model will be evaluated based on its\n",
    "            predictions for this target variable.\n",
    "        visualize : bool, default True\n",
    "            If True, comparison between predictions and true values is visualized.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mse, mae, r2, r2_adj : float\n",
    "            The numerical values of the model's performance metrics mentioned above.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The R2 coefficient is usually a value between 0 and 1, in which 0 indicates that the model explains\n",
    "        none of the variability and 1 indicates that the model explains all the variability. When there is more\n",
    "        than 1 feature however, it is possible for R2 coefficient to be negative if the fit is particularly bad.\n",
    "        - For multiple linear regression, the R2 coefficient is not a good performance metric, as increasing the\n",
    "        number of features always increases the R2 value. The adjusted R2 value (r2_adj) is a metric that accounts\n",
    "        for this \"inflation\" of the R2 value when the number of feature variables increases, and is therefore\n",
    "        more appropriate for evaluating model fit.\n",
    "        \"\"\"\n",
    "\n",
    "        self._target_valid(target)\n",
    "        df_features = self.test_features\n",
    "        df_targets = self.test_targets\n",
    "\n",
    "        y_hat = self.predict(df_features, target)\n",
    "        y = self._prepare_y(df_targets.loc[:, [target]])\n",
    "        if visualize == True:\n",
    "            self._visualize(df_features, df_targets, target, y_hat)\n",
    "        mse, mae, r2, r2_adj = self._calc_metrics(y_hat, y)\n",
    "        print(f\"Mean Square Error: {mse:.5f} | Mean Absolute Error: {mae:.5f} | R2 Coefficient of Determination: {r2:.5f} | Adjusted R2: {r2_adj:.5f}\")\n",
    "        return mse, mae, r2, r2_adj\n",
    "\n",
    "\n",
    "    \"\"\"Helper (private) functions\"\"\"\n",
    "\n",
    "    def _visualize(self, df_features, df_targets, target, y_hat, size=5, n=3):\n",
    "        \"\"\"Visualize comparison between model prediction and true target values with scatter plots.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_features, df_targets : pandas.DataFrame\n",
    "            Dataframes containing the features and targets.\n",
    "        target : str\n",
    "            Column name of target variable being predicted.\n",
    "        y_hat : numpy.ndarray\n",
    "            NumPy array containing model's predictions.\n",
    "        size: int, default 5\n",
    "            Determines the size of the scatterplots.\n",
    "        n : int, default 3\n",
    "            Maximum number of scatterplots in a row.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare data for plots (everything should be 1-dimensional)\n",
    "        num_features = len(self.features)\n",
    "        y = df_targets[target]\n",
    "        y_hat = y_hat.flatten()\n",
    "\n",
    "        # Calculate the number of rows and columns for subplots (n plots per row)\n",
    "        num_rows = int(np.ceil(num_features / n))\n",
    "        num_cols = min(num_features, n)\n",
    "        width = size * num_cols\n",
    "        height = size * num_rows\n",
    "\n",
    "        # Set up subplots\n",
    "        fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(width, height))\n",
    "        if num_features > 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes]\n",
    "\n",
    "        # Subplot style parameters\n",
    "        y_params = {\n",
    "            'color': 'blue',\n",
    "            'marker': 'o',\n",
    "            'label': \"True values\"\n",
    "        }\n",
    "\n",
    "        y_hat_params = {\n",
    "            'color': 'orange',\n",
    "            'marker': 'o',\n",
    "            'label': \"Predictions\"\n",
    "        }\n",
    "\n",
    "        # Plot target against each feature\n",
    "        for i in range(num_features):\n",
    "            feature = self.features[i]\n",
    "            x = df_features[feature]\n",
    "            sns.scatterplot(x=x, y=y, ax=axes[i], **y_params)\n",
    "            sns.scatterplot(x=x, y=y_hat, ax=axes[i], **y_hat_params)\n",
    "\n",
    "            # Add labels and title\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel(target)\n",
    "            axes[i].set_title(\"  \")\n",
    "            # axes[i].set_title(f\"Scatter plot of {target} against {feature}\")\n",
    "            axes[i].legend()\n",
    "\n",
    "        # Hide remaining empty subplots\n",
    "        for j in range(i+1, len(axes)):\n",
    "            axes[j].set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _calc_metrics(self, y_hat, y):\n",
    "        \"\"\"Calculate 4 metrics for model evaluation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_hat, y : numpy.ndarray\n",
    "            Arrays containing model's predictions (y_hat) and the true values (y)\n",
    "            for m training examples. Both should be of shape (m,1).\n",
    "        \"\"\"\n",
    "\n",
    "        r2 = self.R2(y_hat, y)\n",
    "        r2_adj = self.adjusted_R2(r2, y.shape[0], len(self.features))\n",
    "        mse = self.mean_squared_error(y_hat, y)\n",
    "        mae = self.mean_abs_error(y_hat, y)\n",
    "        return mse, mae, r2, r2_adj\n",
    "\n",
    "\n",
    "    def _target_valid(self, target):\n",
    "        \"\"\"Checks whether given target name is valid (found in stored data)\n",
    "        and whether the model has been trained to predict it yet.\"\"\"\n",
    "\n",
    "        if target not in self.targets:\n",
    "            raise ValueError(\"Specified target not found in stored data.\")\n",
    "        elif target not in self.params.keys():\n",
    "            raise ValueError(\"Model has not been trained to predict this target variable yet. Run the train() method.\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _calc_linreg(X, params):\n",
    "        \"\"\"Calculates linear regression equation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy.ndarray\n",
    "            Array of shape (m,n+1) where m is the number of data examples (rows)\n",
    "            and n is the number of feature variables (excluding the intercept term).\n",
    "        params : numpy.ndarray\n",
    "            Array of shape (n+1,1) i.e. column vector where the ith value is the\n",
    "            coefficient for the ith feature (the 0th coefficient is for intercept term)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of shape (m,1).\n",
    "        \"\"\"\n",
    "\n",
    "        return np.matmul(X, params)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_data(df_features, df_targets, random_state, test_size):\n",
    "        \"\"\"Splits the given feature and target dataframes into a test\n",
    "        and train set randomly.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_features, df_targets : pandas.DataFrame\n",
    "            DataFrames to be split into test and train sets.\n",
    "        random_state : int\n",
    "            Integer representing a seed for splitting of the data into training and test sets.\n",
    "            Setting a seed to a specific value ensures reproducibility. If parameter is set to None\n",
    "            instead of an integer, no seed will be specified and data splitting will be random.\n",
    "        test_size : float\n",
    "            Fraction (between 0 to 1) of the data to be allocated as the test set.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df_features_train, df_targets_train : pandas.DataFrame\n",
    "            Features and targets for training set.\n",
    "        df_features_test, df_targets_test : pandas.DataFrame\n",
    "            Features and targets for test set.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the random seed\n",
    "        if random_state is not None:\n",
    "          np.random.seed(random_state)\n",
    "\n",
    "        indexes = df_features.index\n",
    "        k = int(len(indexes) * test_size)\n",
    "\n",
    "        # Randomly shuffle the indices to create a random test set\n",
    "        test_indexes = np.random.choice(indexes,k,replace=False)\n",
    "        train_indexes = list(set(indexes) - set(test_indexes))\n",
    "        df_features_train = df_features.loc[train_indexes, :]\n",
    "        df_features_test = df_features.loc[test_indexes, :]\n",
    "        df_targets_train = df_targets.loc[train_indexes, :]\n",
    "        df_targets_test = df_targets.loc[test_indexes, :]\n",
    "\n",
    "        return df_features_train, df_features_test, df_targets_train, df_targets_test\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_z(dfin, columns_means=None, columns_stds=None):\n",
    "        \"\"\"Normalize input dataframe using Z normalization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfin : pandas.DataFrame\n",
    "            Input data to be normalized.\n",
    "        columns_means, columns_stds : float, optional\n",
    "            Mean and standard deviation values used for normalization. If not\n",
    "            specified (default None), the values will be computed using the input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dfout : pandas.DataFrame\n",
    "            DataFrame containing normalized data.\n",
    "        columns_means, columns_stds : float\n",
    "            Will return the mean and standard deviation calculated using input DataFrame\n",
    "            or the values passed in as arguments (depending on whether values for\n",
    "            columns_means and columns_stds were passed in).\n",
    "        \"\"\"\n",
    "\n",
    "        if columns_means is None:\n",
    "            columns_means = dfin.mean(axis=0) # mean of all rows in each column\n",
    "        if columns_stds is None:\n",
    "            columns_stds = dfin.std(axis=0)\n",
    "\n",
    "        dfout = (dfin - columns_means) / columns_stds # broadcasting\n",
    "        return dfout, columns_means, columns_stds\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_X(df_features):\n",
    "        \"\"\"Turns DataFrame of feature values into design matrix X.\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Takes in DataFrame with shape (m,n) where m is the number of data\n",
    "        examples and n is the number of features. It will be converted into\n",
    "        a 2-dimensional NumPy array and a column of 1s (x0) will be added for\n",
    "        the intercept term.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_features : pandas.DataFrame\n",
    "            DataFrame of feature values with shape (m,n). (If a pandas.Series is passed\n",
    "            in instead, it will be converted to pandas DataFrame automatically.)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X : numpy.ndarray\n",
    "            NumPy array representing design matrix X with shape (m,n+1).\n",
    "        \"\"\"\n",
    "\n",
    "        if type(df_features) == pd.Series:\n",
    "            df_features = pd.DataFrame(df_features)\n",
    "\n",
    "        rows, cols = df_features.shape\n",
    "        if type(df_features) == pd.DataFrame:\n",
    "            features = df_features.to_numpy() # Convert dataframe to np array\n",
    "        else:\n",
    "            features = df_features\n",
    "        features = features.reshape(-1, cols) # ensure it is 2-dimensional array\n",
    "\n",
    "        # Add new column of features x0 = 1 (intercept term):\n",
    "        x0 = np.ones((rows,1))\n",
    "        X = np.concatenate((x0, features), axis=1)\n",
    "        return X # (m, n+1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_y(df_target):\n",
    "        \"\"\"Turns DataFrame of target values into column vector y.\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Takes in DataFrame with shape (m,1) where m is the number of data\n",
    "        examples. It will be converted into a 2-dimensional NumPy array with shape (m,1).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_target : pandas.DataFrame\n",
    "            DataFrame of feature values with shape (m,1). (If a pandas.Series is passed\n",
    "            in instead, it will be converted to pandas DataFrame automatically.)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : numpy.ndarray\n",
    "            NumPy array representing column vector y with shape (m,1).\n",
    "        \"\"\"\n",
    "\n",
    "        if type(df_target) == pd.Series:\n",
    "            df_target = pd.DataFrame(df_target)\n",
    "\n",
    "        cols = df_target.shape[1]\n",
    "        if type(df_target) == pd.DataFrame:\n",
    "            target = df_target.to_numpy()\n",
    "        else:\n",
    "            target = df_target\n",
    "\n",
    "        target = target.reshape(-1, cols) # reinforce that targets should be 2-d np array\n",
    "        return target\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_hat, y):\n",
    "        \"\"\"Compute Mean Squared Error (MSE).\"\"\"\n",
    "\n",
    "        mse =  np.mean(np.square(y_hat - y))\n",
    "        return mse\n",
    "\n",
    "        '''# Alternatively, using matrices:\n",
    "        m = y.shape[0] # number of data examples\n",
    "        err = y_hat - y\n",
    "        mse = np.matmul(err.T, err) / m # no need half factor\n",
    "        mse = mse[0][0]\n",
    "        # is matrix approach better? check which is computationally faster\n",
    "        '''\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def R2(y_hat, y):\n",
    "        \"\"\"Compute R-squared value.\"\"\"\n",
    "\n",
    "        y_bar = np.mean(y)\n",
    "        err = y_hat - y\n",
    "        ss_residual = np.sum(np.square(y - y_hat))\n",
    "        ss_total = np.sum(np.square(y - y_bar)) # y_bar is broadcasted\n",
    "        r2 = 1 - (ss_residual / ss_total)\n",
    "        # It is possible for r2 to be negative (ss_res > ss_total) if not model not well fitted.\n",
    "        return r2\n",
    "\n",
    "        '''# Alternatively, using matrices:\n",
    "        # err = y_hat - y\n",
    "        # ss_res = np.matmul(err.T, err)\n",
    "        # ss_tot = np.matmul((y - y_bar).T, (y - y_bar))\n",
    "        '''\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def adjusted_R2(r2, num_examples, num_features):\n",
    "        \"\"\"Compute Adjusted R-squared value.\"\"\"\n",
    "\n",
    "        return 1 - ( (1-r2)*(num_examples-1) ) / (num_examples-num_features-1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_abs_error(y_hat, y):\n",
    "        \"\"\"Compute Mean Absolute Error (MAE).\"\"\"\n",
    "\n",
    "        return np.mean(np.abs(y_hat - y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Prepare data for model\n",
    "In the 'Features and Target Preparation' section, we have already prepared our pandas dataframe with all our data and chosen our feature and target variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target:\", target) # chosen target variable\n",
    "print(\"Chosen features:\", chosen_features) # chosen feature variables\n",
    "df # entire DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Prepare model\n",
    "Instantiate Multiple Linear Regression model and load data into it. Indicate chosen target and feature variables. As noted in the class documentation, the model will split the dataset into a test set and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MultipleLinearRegression()\n",
    "model1.store_data(df, chosen_features, [target], random_state=100, test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train model\n",
    "Note: Because the 'retrain' parameter of the train method is set to False by default, if you run the cell multiple times, the model will start training using its currently saved parameter values (obtained from the previous training). Hence, why the training cost curve is different each time you run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.train(target, alpha=0.1, epochs=25)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "*Describe your metrics and how you want to evaluate your model. Put any Python code to evaluate your model. Use plots to have a visual evaluation.*\n",
    "<br>\n",
    "Our class has a method called 'validate', in which the model will use the test set to evaluate its performance by comparing its predictions to the true target values. This method computes and returns four performance metrics that can be used to evaluate the model:\n",
    "1) Mean Square Error (MSE): it is the average squared difference between the predicted and actual values. While MSE is useful for gradient descent as a cost function, it should be noted that MSE is extremely sensitive to outliers, as large deviations between predicted and actual values are made even larger when squared. Additionally, MSE does not provide any insight on the directionality of the deviation (i.e. whether the model underpredicts or overpredicts).\n",
    "2) Mean Absolute Error (MAE): the average absolute difference between the predicted and actual values. MAE is less sensitive to outliers than MSE as the deviations are not squared but also similarly does not provide insight on directionality.\n",
    "3) R2 coefficient of determination (not considered): a value *typically* between 0 and 1 that provides a measure of goodness of fit of the model as it is the proportion of variance in the dependent variable that is explained by the model. An R2 value of 1 indicates that the model perfectly explains the variance while 0 means the model does not explain any of the variance. **However**, in the case where more than 1 feature is considered, it is possible for R2 to be a negative value if the model has a particularly terrible fit. Furthermore, R2 is not a useful measure when there is more than 1 feature, as its value will with more features (predictors). Hence, we will not be considering R2 when evaluating our model.\n",
    "4) **Adjusted** R2 coefficient of determination: the adjusted R2 coefficient is essentially the R2 coefficient except that it takes into account the number of features in the model. This makes it a more meaningful metric for evaluating our multiple linear regression model, in which there may be multiple features (and allows us to meaningfully compare models with different number of features). \n",
    "<br>\n",
    "Generally, MSE and MAE are measures of the model's prediction accuracy, while adjusted R2 is a measure of model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model and visualize its accuracy\n",
    "model1.validate(target)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as can be seen in the scatter plots, our model's predictions are very off. Our adjusted R2 is even negative, which can happen in the multiple linear regression case if the model is very badly fitted. We will be improving our model's performance in the following steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the Model\n",
    "\n",
    "\"Discuss any steps you can do to improve the models. Put any python codes. You can repeat the steps above with the codes to show the improvement in the accuracy.\"\n",
    "\n",
    "In this section, we will be improving our model's performance. We have already taken some steps towards this previously, by cleaning our data (e.g. removing outliers that are essentially noise) and choosing our features carefully. Now, we will be attempting to improve our model with the following approaches:\n",
    "1) Hyperparameter Tuning\n",
    "2) Feature transformation\n",
    "3) Weighted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: Hyperparameter Tuning\n",
    "Thanks to our class-based implementation, we can create different models trained with varying configurations (different hyperparameter values) and compare model performance. The hyperparameters we can tune are:\n",
    "- alpha: the learning rate, which determines the size of the step taken during gradient descent\n",
    "- epochs: the number of iterations during training in gradient descent i.e. the number of steps taken\n",
    "- test_size: the fraction of our data set used for testing (as opposed to training)\n",
    "\n",
    "To avoid cluttering this file, we have conducted hyperparameter tuning elsewhere and have landed on the following hyperparameters values used in model2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MultipleLinearRegression()\n",
    "model2.store_data(df, chosen_features, [target], random_state=100, test_size=0.5)\n",
    "model2.train(target, alpha=0.01, epochs=600)\n",
    "model2.validate(target)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 hyperparameters: test_size = 0.9, alpha = 0.1, epochs = 25\n",
    "<br>model2 hyperparameters: test_size = 0.5, alpha = 0.01, epochs = 600\n",
    "\n",
    "As you can see from the results above, model2 is significantly more accurate in its predictions than model1. It has lower mean square error and mean absolute error. Its adjusted R2 coefficient is also significantly better. In our investigation on the hyperparameters, we have observed the following:\n",
    "- **alpha** (learning rate): In general, a good value is somewhere between 0.1 and 0.01. Using smaller values means that more iterations are needed to converge to the cost function minima and therefore training would require more time (which is a non-issue for our small-scale project). If the learning rate is too high (e.g. 1), it is possible for the cost to increase during training instead. This happens because the gradient descent step becomes too big and the parameter values overshoot past the minima, causing it to diverge from rather than converge to minima. As a future improvement to our model, learning rate scheduling can be implemented, in which the learning rate starts off large (for quicker convergence) but gets much smaller when the parameters are closer to the minima (to avoid divergence with exploding gradients).\n",
    "- **epochs**: in general, the higher the better - the more iterations of gradient descent, the lower the cost function gets. However, as can be seen in the cost function plot above, the rate at which the cost decreases diminishes significantly after a certain point and therefore there is a trade-off between time and accuracy.\n",
    "- **test_size**: if too much of the data set is allocated to the test set, there may be insufficient data for training, which may result in the model being unable to pick up on the general trends in the overall data. A small training set is inadequate in representing the complexity of the data and the model is more likely to overfit to the small training set. It is also more sensitive to noise. On the other hand, if the test size is too small, the evaluation metrics may not be as reliable there aren't enough predictions and the evaluation metric becomes too sensitive to the specific data points chosen for testing, resulting in more varied and less reliable estimations of model performance. We found a test_size value of around 0.5 to have a good balance between the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Feature Engineering\n",
    "Feature engineering involves transforming our input features into more relevant forms for our model.\n",
    "One example of feature engineering that has already been implemented is the normalization of input features, which is automatically done by the model (our class instance) during training and prediction. Input features are normalized using z-normalization (through the private _normalize_z method in the class).<br>\n",
    "\n",
    "Another method is to transform our features, changing their representation in a way to make them more suitable for multiple linear regression. Our class has a method called 'add_transformed_feature' which will transform a chosen feature. The transformed feature can either replace the old feature or be added as a new feature.<br>\n",
    "*Note: a model should always be retrained after adding a new transformed feature.*\n",
    "<br>\n",
    "As can be seen in our previous scatter plots, the features 'World Freedom Index' and 'Cost of Living Index' seem to fit non-linearly to our target. In the cell below, we will carry out a polynomial transformation of these features (square our feature values) and have them replace their non-transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new model with the same hyperparameters as model2\n",
    "model3 = MultipleLinearRegression()\n",
    "model3.store_data(df, chosen_features, [target], random_state=100, test_size=0.5) # pass in seed to make data split the same\n",
    "\n",
    "# Before training, let's transform our features\n",
    "model3.add_transformed_feature(\"World Freedom Index\", \"WFI Square\", np.square, replace=True)\n",
    "model3.add_transformed_feature(\"Cost of Living Index\", \"CLI Square\", np.square, replace=True)\n",
    "\n",
    "# Train and validate model3\n",
    "model3.train(target, alpha=0.01, epochs=600, show=False)\n",
    "model3.validate(target)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, our model has only improved very slightly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3: Locally Weighted Regression\n",
    "Previously, we attempted to transform our features to have them fit linearly with the target. However, sometimes it is difficult to determine what transformations should be used. We attempt to sidestep this issue by using locally weighted regression instead. Locally weighted regression, also known as LOESS, is a non-parametric variation of multiple linear regression that has several benefits. Unlike normal multiple linear regression, LOESS makes no assumptions of global linearity and can flexibly capture non-linearity. LOESS is similar to normal linear regression except that instead of minimising mean square error, it instead minimises locally weighted mean square error. In short, training examples nearer to the point of prediction are weighted higher than those further away. This makes LOESS adaptive to local patterns and less sensitive to outliers.\n",
    "<br>\n",
    "However, LOESS isn't without its issues. Finding an appropriate value for the bandwidth parameter, which affects the spread of the gaussian kernel (the weight), is both important (due to the impact it has on model performance) and challenging. Furthermore, LOESS is more computationally intensive in that it has to compute the optimal parameter values for every prediction (as we have to minimise locally weighted cost). \n",
    "<br>\n",
    "For this project, we have implemented LOESS as a subclass of our MultipleLinearRegression class. This allows us to inherit previously defined methods while making certain changes for LOESS. The main difference is that LOESS does not have a training step because optimal parameter values are obtained for each prediction (and hence the model does not store parameter values). See the class documentation below for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocallyWeightedRegression(MultipleLinearRegression):\n",
    "    \"\"\"Class for a locally weighted regression model.\n",
    "\n",
    "    Extended Summary\n",
    "    ----------------\n",
    "    The model is a subclass of MultipleLinearRegression and inherits most\n",
    "    of its functionality. Unlike in multiple linear regression, however, this\n",
    "    model does not save any parameter values and are instead computed everytime\n",
    "    the model makes a prediction (as it needs to fit to the locally weighted cost).\n",
    "    Hence there is no training step. After training data is stored, the model can\n",
    "    make predictions immediately.\n",
    "    General user flow:\n",
    "    1) Store data (expects pandas DataFrames) using store_data().\n",
    "    2) Validate the model's performance using validate().\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    features, targets : list of str\n",
    "        A list of strings containing the column names of the features and targets respectively.\n",
    "        Initialised when data is stored in the instance using store_data().\n",
    "    train_features, test_features, train_targets, test_targets : pandas.DataFrames\n",
    "        Pandas Dataframes containing the features/targets for training/testing. Initialised\n",
    "        when data is stored in the instance using store_data().\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    Inherited:\n",
    "        store_data(data, feature_names, target_names, random_state=None, test_size=0.5)\n",
    "            Takes the given data and separates the features and targets. Then splits the data\n",
    "            for training and testing before storing it in the model's attributes.\n",
    "        add_transformed_feature(feature_name, new_feature_name, transform_func, replace=False)\n",
    "            Transforms a specified feature and adds it to the stored data.\n",
    "\n",
    "    Overriden:\n",
    "        train()\n",
    "            This method is disabled. Parameter values are obtained for each\n",
    "            prediction in locally weighted regression.\n",
    "        predict(dfin_features, target, tau=1)\n",
    "            Given some input data of features, the model returns its predictions for the specified target\n",
    "            variable using locally weighted regression.\n",
    "        validate(target, visualize=True, tau=1)\n",
    "            Evaluate the model's ability to predict the specified target variable (its performance) using the\n",
    "            test data. Will show visualizations of the predicted values against the true values for the test data\n",
    "            and return various performance metrics.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Locally weighted regression is a non-parametric learning algorithm. The bandwidth parameter (tau) is very\n",
    "    important value that has a significant impact on the model's performance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructs a new instance of LocallyWeightedRegression.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Unlike the MultipleLinearRegression class, there is no need to store\n",
    "        parameters nor the mean and standard deviation of the training features.\n",
    "        In locally weighted regression, the parameter values are determined for\n",
    "        each prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        self.features, self.targets = None, None # list of names\n",
    "        self.train_features, self.train_targets = None, None\n",
    "        self.test_features, self.test_targets = None, None\n",
    "\n",
    "    def predict(self, dfin_features, target, tau=1):\n",
    "        \"\"\"Predict for the specified target with the given input features using\n",
    "        locally weighted regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dfin_features : pandas.DataFrame\n",
    "            Data containing input features to predict with, where each row is a set of input features (x).\n",
    "            It is a DataFrame with shape (m,n), where m is the number of data points to be predicted\n",
    "            and n is the number of feature variables.\n",
    "        target : str\n",
    "            Column name of target variable to be predicted.\n",
    "        tau : float, default 1\n",
    "            Bandwidth parameter value of the gaussian kernel (local weight).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_hat : numpy.ndarray\n",
    "            NumPy array that has a shape of (m,1) and contains the model's predictions for each\n",
    "            of the input data points.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        In locally weighted regression, the parameters for each prediction changes depending on the\n",
    "        input features. Datapoints closer to the input feature values (point of prediction) are given\n",
    "        more weightage than points further away. The bandwidth parameter (tau) determines the spread of\n",
    "        the gaussian kernel (which determines the weightages of points). A smaller bandwidth parameter\n",
    "        means a more concentrated gaussian kernel, making it more sensitive to neighbouring points as opposed\n",
    "        to more distant ones.\n",
    "        When computing the parameters using the matrix equation, this function finds the Moore-Penrose inverse\n",
    "        rather than the regular inverse. This is because the square matrix in the equation (X_train.T @ W @ X_train)\n",
    "        may be singular and therefore non-invertible. In this case, the Moore-Penrose inverse is sufficient for\n",
    "        the purpose of optimising the locally-weighted cost function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check that there is data stored for training and validation:\n",
    "        if any(data is None for data in (self.train_features, self.train_targets, self.test_features, self.test_targets)):\n",
    "            raise ValueError(\"Data missing. Run the store_data() method first.\")\n",
    "        # Check that valid target name is specified:\n",
    "        elif target not in self.targets:\n",
    "            raise ValueError(\"Specified target not found in stored data.\")\n",
    "\n",
    "        # Prepare training features and target:\n",
    "        train_features_norm, means, stds = self._normalize_z(self.train_features)\n",
    "        X_train = self._prepare_X(train_features_norm)\n",
    "        y = self._prepare_y(self.train_targets.loc[:, [target]])\n",
    "\n",
    "        # Normalize and prepare input features\n",
    "        test_features_norm, _, _ = self._normalize_z(dfin_features, means, stds)\n",
    "        X_in = self._prepare_X(test_features_norm)\n",
    "\n",
    "        num_inputs = X_in.shape[0]\n",
    "        y_hat = np.zeros(num_inputs) # for storing predictions\n",
    "\n",
    "        # Make prediction for each input data (row in X_in):\n",
    "        for i in range(num_inputs):\n",
    "            # x is specific point at which you want to make prediction\n",
    "            x = X_in[i,:] # (n,) 1D array for broadcasting purposes\n",
    "\n",
    "            dist = np.sum((X_train - x)**2, axis=1)\n",
    "            w = np.exp(-dist / (2 * tau**2)) # (m,)\n",
    "            W = np.diag(w) # diagonal matrix (m,m)\n",
    "            params = np.linalg.pinv(X_train.T @ W @ X_train) @ (X_train.T @ W @ y) # Moore-Penrose inverse\n",
    "            pred = x.reshape(1,-1) @ params\n",
    "            y_hat[i] = pred\n",
    "\n",
    "        y_hat = y_hat.reshape(-1,1)\n",
    "        return y_hat # (m,1)\n",
    "\n",
    "    def validate(self, target, visualize=True, tau=1):\n",
    "        \"\"\"Evaluate the model's performance using testing data and return performance metrics.\n",
    "\n",
    "        Extended Summary\n",
    "        ----------------\n",
    "        Using the testing data, the model will be evaluated by comparing its predictions (of the\n",
    "        specified target variable) with the true target values. This comparison is visualised with\n",
    "        scatter plots of the target variable against each input feature. The following metrics, which\n",
    "        evaluate the multiple linear regression model's predictions, are computed and returned:\n",
    "        - Mean Square Error\n",
    "        - Mean Absolute Error\n",
    "        - R2 coefficient of determination\n",
    "        - Adjusted R2 coefficient of determination\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : str\n",
    "            Column name of target variable to be predicted. The model will be evaluated based on its\n",
    "            predictions for this target variable.\n",
    "        visualize : float, default True\n",
    "            If True, comparison between predictions and true values is visualized.\n",
    "        tau : float, default 1\n",
    "            Bandwidth parameter value of the gaussian kernel (local weight).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mse, mae, r2, r2_adj : float\n",
    "            The numerical values of the model's performance metrics mentioned above.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        - The R2 coefficient is usually a value between 0 and 1, in which 0 indicates that the model explains\n",
    "        none of the variability and 1 indicates that the model explains all the variability. When there is more\n",
    "        than 1 feature however, it is possible for R2 coefficient to be negative if the fit is particularly bad.\n",
    "        - For multiple linear regression, the R2 coefficient is not a good performance metric, as increasing the\n",
    "        number of features always increases the R2 value. The adjusted R2 value (r2_adj) is a metric that accounts\n",
    "        for this \"inflation\" of the R2 value when the number of feature variables increases, and is therefore\n",
    "        more appropriate for evaluating model fit.\n",
    "        \"\"\"\n",
    "\n",
    "        self._target_valid(target)\n",
    "        df_features = self.test_features\n",
    "        df_targets = self.test_targets\n",
    "\n",
    "        y_hat = self.predict(df_features, target, tau=tau)\n",
    "        if visualize == True:\n",
    "            self._visualize(df_features, df_targets, target, y_hat)\n",
    "        y = self._prepare_y(df_targets.loc[:, [target]])\n",
    "        mse, mae, r2, r2_adj = self._calc_metrics(y_hat, y)\n",
    "        print(f\"Mean Square Error: {mse:.5f} | Mean Absolute Error: {mae:.5f} | R2 Coefficient of Determination: {r2:.5f} | Adjusted R2: {r2_adj:.5f}\")\n",
    "        return mse, mae, r2, r2_adj\n",
    "\n",
    "\n",
    "    def _target_valid(self, target):\n",
    "        \"\"\"Checks whether given target name is valid (found in stored data)\"\"\"\n",
    "\n",
    "        if target not in self.targets:\n",
    "            raise ValueError(\"Specified target not found in stored data.\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def train():\n",
    "        raise Exception(\"This method is non-functional for Locally Weighted Regression. Parameter values are computed during each prediction instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new model with the same hyperparameters as model2\n",
    "loess_model = LocallyWeightedRegression()\n",
    "loess_model.store_data(df, chosen_features, [target], random_state=100, test_size=0.5) # pass in seed to make data split the same\n",
    "\n",
    "# Validate LOESS model\n",
    "loess_model.validate(target, tau=3.5)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the performance of our model, while still decent, has decreased. Locally Weighted Regression is not the way to go for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extra demonstration of LOESS\n",
    "However, just to demonstrate that locally weighted regression can be useful, this section will show the performance difference between a normal linear regression model and a LOESS model using housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Linear Regression model\n",
    "df_housing = pd.read_csv(\"housing_processed.csv\")\n",
    "housing_model_mlr = MultipleLinearRegression()\n",
    "housing_model_mlr.store_data(df_housing, [\"RM\",\"DIS\",\"INDUS\"], [\"MEDV\"], random_state=100, test_size=0.5)\n",
    "housing_model_mlr.train(\"MEDV\", show=False)\n",
    "housing_model_mlr.validate(\"MEDV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally Weighted Regression Model\n",
    "housing_model_loess = LocallyWeightedRegression()\n",
    "housing_model_loess.store_data(df_housing, [\"RM\",\"DIS\",\"INDUS\"], [\"MEDV\"], random_state=100, test_size=0.5)\n",
    "housing_model_loess.validate(\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOESS model has much better performance metric values. The effect that locally weighted regression has is very apparent in the scatterplot of MEDV against RM. In the normal multiple linear regression model, the predictions (in orange) follow a very strict line. In the locally weighted regression model however, the predictions are less linear and follow the true values more closely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion and Analysis\n",
    "\n",
    "*Discuss your model and accuracy in solving the problem. Analyze the results of your metrics. Put any conclusion here.*\n",
    "<br>\n",
    "Out of all our models, model3 has the best performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.validate(target)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, our model has a fairly low mean square error and mean absolute error which means our model is decently accurate in its predictions. Meanwhile, the model's adjusted R2 coefficient is 0.56045, which suggests that it explains slightly more than half of the variance in the dependent variable (target). This means that the model has moderate explanatory power but there is much room for improvement.\n",
    "<br>\n",
    "The biggest limitation of this model is the lack of data points. Intuitively, more data results in a better representation of the population. The low data count also made our model particularly susceptible to high variability in performance due to the splitting of data. For example, the randomly chosen datapoints for the training set may not form a good representation of the overall population and may result in the model learning less than ideal parameter values. Again, this issue is mitigated with more data points. We also suspect that the poor results of the locally weighted regression model might be similarly due to this lack of data, as it performed much better with the housing dataset (which has a lot more values). \n",
    "<br> This project also emphasised the importance of feature engineering as it can be seen that many of the variables do not fit linearly with the target variable (hence why we attempted to use locally weighted regression). With that all in mind, we identified some possible future improvements:\n",
    "1) Obtain more data points, possibly ignore variables that limit the number of data points available\n",
    "2) More thorough data cleaning; remove outliers and noise (as multiple linear regression is particularly susceptible to noise due to using MSE as a cost function)\n",
    "3) Introduce regularization parameters to combat overfitting (especially if we introduce many polynomial features)\n",
    "4) More stringent feature selection: we chose our selection by looking at the r2 coefficient between a particular feature and the target, but never considered looking at the correlation between the target and transformed versions of the feature. We did not do this due to time constraints as there are many transformations that can be applied and many possible feature variables to look at."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
